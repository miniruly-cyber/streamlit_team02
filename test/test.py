# app_v12_resume_coach_plus.py
# =========================================================
# AI ìê¸°ì†Œê°œì„œ ì½”ì¹­ - PLUS (í‰ê°€ + íŠ¸ë Œë“œ/ê¸°ì—… ì¸ì¬ìƒ ìˆ˜ì§‘)
# =========================================================
# ì„¤ì¹˜(ê¶Œì¥):
#   pip install -U pip
#   pip install streamlit langchain langchain-openai python-docx reportlab python-dotenv \
#               pandas numpy altair plotly requests beautifulsoup4 tiktoken
# ì‹¤í–‰:
#   streamlit run app_v12_resume_coach_plus.py
# ---------------------------------------------------------
# ì„ íƒ í™˜ê²½ë³€ìˆ˜ (.env ë˜ëŠ” í„°ë¯¸ë„ export)
#   OPENAI_API_KEY=...
#   SERPAPI_API_KEY=...           # (ì„ íƒ) ì›¹ ë™í–¥/ì¸ì¬ìƒ ê²€ìƒ‰ìš©
#   BING_API_KEY=...              # (ì„ íƒ) Bing Web Search API
#   DATA_DIR=./data               # (ì„ íƒ) CSV ì €ì¥ ê²½ë¡œ (ê¸°ë³¸: /mnt/data ê°€ ìš°ì„ )
# =========================================================

import os, io, re, json, textwrap, datetime, time
from typing import Optional, List, Dict, Tuple

import streamlit as st

# ===== Optional libs =====
try:
    import pandas as pd
    import numpy as np
    PANDAS_OK = True
except Exception:
    PANDAS_OK = False

try:
    import altair as alt
    import plotly.express as px
    VIZ_OK = True
except Exception:
    VIZ_OK = False

try:
    import requests
    from bs4 import BeautifulSoup
    HTTP_OK = True
except Exception:
    HTTP_OK = False

# ===== ë¬¸ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„ íƒ) =====
try:
    from docx import Document
    DOCX_OK = True
except Exception:
    DOCX_OK = False

try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer
    from reportlab.lib.styles import getSampleStyleSheet
    REPORTLAB_OK = True
except Exception:
    REPORTLAB_OK = False

# ===== LangChain / OpenAI (ì„ íƒ) =====
try:
    from langchain_openai import ChatOpenAI
    from langchain.prompts import ChatPromptTemplate
    from langchain.chains import LLMChain
    LLM_OK = True
except Exception:
    LLM_OK = False

# ================= ì „ì—­ ì„¤ì • =================
st.set_page_config(page_title="AI ìê¸°ì†Œê°œì„œ ì½”ì¹­+", page_icon="ğŸ’¬", layout="wide")

MAIN = "#22C55E"
BG = "#F5FBFB"
USER_BG = "#DCFCE7"
BOT_BG = "#F3F4F6"

st.markdown(
    f"""
    <style>
        body {{ background:{BG}; }}
        .header {{position:sticky; top:0; background:{MAIN}; color:white; padding:10px; z-index:10;}}
        .bubble-user {{background:{USER_BG}; padding:10px; border-radius:16px; margin:6px 0; text-align:right}}
        .bubble-bot {{background:{BOT_BG}; padding:10px; border-radius:16px; margin:6px 0; text-align:left}}
        .metric-box {{border-radius:14px; padding:10px; background:white; border:1px solid #e5e7eb}}
    </style>
    """,
    unsafe_allow_html=True,
)

# ================= ìœ í‹¸ =================
def _env(key: str, default: str="") -> str:
    return os.getenv(key, default)

@st.cache_data(show_spinner=False)
def _default_data_dir() -> str:
    # /mnt/data ê°€ ì¡´ì¬í•˜ë©´ ìš°ì„  ì‚¬ìš© (ChatGPT ì—…ë¡œë“œ ê²½ë¡œì™€ í˜¸í™˜)
    if os.path.isdir("/mnt/data"):
        return "/mnt/data"
    return _env("DATA_DIR", "./data")

DATA_DIR = _default_data_dir()

@st.cache_data(show_spinner=False)
def load_csv(name: str) -> Optional[pd.DataFrame]:
    if not PANDAS_OK:
        return None
    # 1) DATA_DIR, 2) í˜„ì¬ê²½ë¡œ ìˆœìœ¼ë¡œ íƒìƒ‰
    candidates = [os.path.join(DATA_DIR, name), os.path.join(".", name)]
    for path in candidates:
        if os.path.isfile(path):
            try:
                df = pd.read_csv(path)
                return df
            except Exception:
                pass
    return None

# ================= ë°ì´í„° ë¡œë”© =================
if PANDAS_OK:
    job_market = load_csv("job_market.csv")
    macro = load_csv("macro_indicators.csv")
    skills = load_csv("skills_analysis.csv")
    tech_trends = load_csv("tech_trends.csv")
else:
    job_market = macro = skills = tech_trends = None

# ================= í…ìŠ¤íŠ¸/ë¬¸ì„œ ì²˜ë¦¬ =================

def read_text_from_upload(uploaded) -> str:
    if uploaded is None:
        return ""
    name = uploaded.name.lower()
    try:
        if name.endswith('.txt'):
            return uploaded.read().decode('utf-8', errors='ignore')
        if name.endswith('.docx') and DOCX_OK:
            doc = Document(uploaded)
            return "\n".join(p.text for p in doc.paragraphs)
        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‹œë„
        return uploaded.read().decode('utf-8', errors='ignore')
    except Exception as e:
        return f"[íŒŒì¼ ì½ê¸° ì˜¤ë¥˜] {e}"

# ================= ê·œì¹™ ê¸°ë°˜ ìŠ¤ì½”ì–´ëŸ¬ =================
ACTION_WORDS = [
    "ê°œì„ ", "ìµœì í™”", "ì„¤ê³„", "êµ¬í˜„", "ë¶„ì„", "ìë™í™”", "í˜‘ì—…", "ë¦¬íŒ©í„°", "ê²€ì¦",
    "ì„±ê³¼", "ì¦ê°€", "ê°ì†Œ", "ë‹¬ì„±", "ê¸°ì—¬", "í•´ê²°", "ë¦¬ë”", "ì¡°ìœ¨"
]
STAR_TOKENS = ["ìƒí™©", "ê³¼ì œ", "í–‰ë™", "ê²°ê³¼", "Situation", "Task", "Action", "Result"]
FILLERS = ["ìµœëŒ€í•œ", "ì •ë§", "ë§¤ìš°", "ë‹¤ì–‘í•œ", "ë§ì€", "ì—´ì •", "ì„±ì‹¤", "ë…¸ë ¥"]

NUM_RE = re.compile(r"(?<!\w)(?:[0-9]+(?:\.[0-9]+)?%?|[ì¼ì´ì‚¼ì‚¬ì˜¤ìœ¡ì¹ íŒ”êµ¬ì‹­]+%?)(?!\w)")

def tokenize_kr(text: str) -> List[str]:
    # ê°„ë‹¨ í† í°í™”(ê³µë°± ê¸°ì¤€). í˜•íƒœì†Œë¶„ì„ê¸° ì—†ì´ ë™ì‘
    return re.findall(r"[\wê°€-í£%]+", text.lower())

def skill_coverage(text: str, skills_df: Optional[pd.DataFrame], month: Optional[str]=None) -> Tuple[float, List[str]]:
    if skills_df is None or len(skills_df) == 0:
        return 0.0, []
    toks = set(tokenize_kr(text))
    # ìµœì‹  ì›” ìš°ì„ 
    df = skills_df.copy()
    if month and 'month' in df.columns:
        df = df[df['month'] == month] if (df['month'] == month).any() else df
    # ìƒìœ„ ê¸°ìˆ  ìƒê´€ì—†ì´ ì „ì²´ ê¸°ìˆ  ê¸°ì¤€ìœ¼ë¡œ ì»¤ë²„ë¦¬ì§€ í‰ê°€
    listed = [str(s).lower() for s in df['skill'].unique().tolist()]
    matched = sorted({s for s in listed if any(s in t for t in toks)})
    cov = len(matched) / max(1, len(set(listed)))
    return cov, matched[:20]

def compute_resume_scores(text: str, role: str = "", company: str = "", skills_df: Optional[pd.DataFrame]=None) -> Dict[str, float]:
    tokens = tokenize_kr(text)
    n_words = len(tokens)
    n_chars = len(text)

    # ìˆ«ì(ì„±ê³¼) ë°€ë„
    nums = NUM_RE.findall(text)
    metric_density = min(1.0, len(nums) / max(1, n_words) * 10)  # ëŒ€ëµì  ì •ê·œí™”

    # í–‰ë™ë™ì‚¬/ì•¡ì…˜
    action_hits = sum(1 for w in ACTION_WORDS if any(w in t for t in tokens))
    action_score = min(1.0, action_hits / 6)

    # STAR ë‹¨ì„œ
    star_hits = sum(1 for w in STAR_TOKENS if any(w.lower() in t for t in tokens))
    star_score = min(1.0, star_hits / 4)

    # êµ°ë”ë”ê¸°(ê°ì )
    filler_hits = sum(tokens.count(f.lower()) for f in FILLERS)
    filler_penalty = min(0.3, filler_hits / max(1, n_words) * 5)

    # ê¸¸ì´ ì ì •ì„±(600~1200ì ê¶Œì¥)
    length_score = 1.0 if 600 <= n_chars <= 1200 else max(0.3, 1 - abs(n_chars - 900) / 1200)

    # ìŠ¤í‚¬ ì»¤ë²„ë¦¬ì§€(íŠ¸ë Œë“œ ë°˜ì˜)
    month = None
    if skills_df is not None and 'month' in skills_df.columns:
        month = skills_df['month'].max()
    cov, matched = skill_coverage(text, skills_df, month)
    coverage_score = min(1.0, 0.5 + cov)  # 0.5~1.0

    # ê°€ì¤‘í•©
    weights = {
        'metrics': 0.25,
        'action': 0.15,
        'star': 0.15,
        'length': 0.15,
        'coverage': 0.30,
    }

    total = (
        metric_density * weights['metrics'] +
        action_score * weights['action'] +
        star_score * weights['star'] +
        length_score * weights['length'] +
        coverage_score * weights['coverage']
    )
    total = max(0.0, min(1.0, total - filler_penalty))

    return {
        'ì´ì (0-100)': round(total * 100, 1),
        'ì„±ê³¼(ìˆ«ì)ë°€ë„': round(metric_density, 3),
        'í–‰ë™ì„±': round(action_score, 3),
        'STARêµ¬ì¡°': round(star_score, 3),
        'ê¸¸ì´ì ì •': round(length_score, 3),
        'ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€': round(coverage_score, 3),
        'êµ°ë”ë”ê¸°ê°ì ': round(filler_penalty, 3),
    }

def llm_improve(text: str, role: str, company: str, tone: str, length: int) -> str:
    if not LLM_OK or not os.getenv('OPENAI_API_KEY'):
        return "[LLM ë¯¸ì‚¬ìš©] OpenAI API í‚¤ê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ì„¤ì • íƒ­ì—ì„œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”."
    system = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ìê¸°ì†Œê°œì„œ ì²¨ì‚­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    - í†¤: {tone}
    - ìµœëŒ€ ê¸¸ì´: {length}ì
    - ì‘ì—…: ì•„ë˜ ìê¸°ì†Œê°œì„œë¥¼ {company} {role} ì§€ì› ê¸°ì¤€ìœ¼ë¡œ STAR êµ¬ì¡°ì™€ ìˆ˜ì¹˜ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ë“¬ê³ , ì¤‘ë³µ/êµ°ë”ë”ê¸°ë¥¼ ì¤„ì´ì„¸ìš”.
    - ì¶œë ¥ í˜•ì‹: 1) ê°œì„  ìš”ì•½(ë¶ˆë¦¿) 2) ê°œì„ ëœ ìê¸°ì†Œê°œì„œ(ë¬¸ë‹¨) 3) ë‹¤ìŒ ì•¡ì…˜ 3ê°€ì§€"""
    tmpl = ChatPromptTemplate.from_messages([
        ("system", system),
        ("human", "ì›ë¬¸:\n{orig}")
    ])
    chain = LLMChain(llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.4), prompt=tmpl)
    out = chain.invoke({"orig": text})
    return out.get("text", str(out))

# ================= (NEW) ì±„íŒ…ìš© ê¸°ì—… ë°ì´í„° ìš”ì²­ ì²˜ë¦¬ (UI ë³€ê²½ ì—†ìŒ) =================

COMPANY_CMD_RE = re.compile(
    r"ë‚´ê°€\s*(?P<company>.+?)\s*ì˜?\s*ìì†Œì„œì—\s*ëŒ€í•œ\s*ë°ì´í„°(?:ë¥¼)?\s*ì–»ê³ \s*ì‹¶ì–´",
    re.IGNORECASE
)

def _clean_company(s: str) -> str:
    s = s.strip()
    s = re.sub(r'^[\"â€œâ€â€˜â€™\'(\[]+', "", s)
    s = re.sub(r'[\"â€œâ€â€˜â€™\'\])]+$', "", s)
    return s.strip()

def try_parse_company_query(text: str) -> Optional[str]:
    if not text:
        return None
    m = COMPANY_CMD_RE.search(text)
    if not m:
        return None
    return _clean_company(m.group("company"))

def summarize_company_from_csvs(company: str) -> str:
    """
    ë¡œì»¬ CSVë§Œ ì‚¬ìš©í•´ ê°„ë‹¨ ìš”ì•½. pandas/CSV ì—†ìœ¼ë©´ ì•ˆë‚´ë§Œ ë°˜í™˜.
    ì´ í•¨ìˆ˜ëŠ” ë¬¸ìì—´ë§Œ ë°˜í™˜í•˜ë¯€ë¡œ ê¸°ì¡´ ë§í’ì„  UIì— ê·¸ëŒ€ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    """
    if not PANDAS_OK:
        return (
            f"### ğŸ“Š ê¸°ì—… ìì†Œì„œ ë°ì´í„° ìš”ì•½ â€” {company}\n"
            "- ì´ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ `pandas` ì„¤ì¹˜ê°€ í•„ìš”í•´ìš”. `pip install pandas` í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n"
        )

    lines = [f"### ğŸ“Š ê¸°ì—… ìì†Œì„œ ë°ì´í„° ìš”ì•½ â€” {company}"]

    # ì±„ìš©ê³µê³  ìš”ì•½
    if job_market is not None:
        sub = job_market.copy()
        if "company" in sub.columns:
            sub = sub[sub["company"].astype(str).str.contains(company, case=False, na=False)]
        # ê³µê³ ìˆ˜
        try:
            cnt = sub["job_code"].nunique() if "job_code" in sub.columns else len(sub)
        except Exception:
            cnt = len(sub)
        # ìµœì‹  ë‚ ì§œ
        recent = ""
        if "posted_date" in sub.columns:
            try:
                _d = pd.to_datetime(sub["posted_date"], errors="coerce")
                if _d.notna().any():
                    recent = _d.max().date().isoformat()
            except Exception:
                pass

        msg = f"- ìµœê·¼ ìˆ˜ì§‘ ê³µê³  ìˆ˜: **{cnt}ê±´**"
        if recent:
            msg += f" (ìµœì‹ : {recent})"
        lines.append(msg)
    else:
        lines.append("- `job_market.csv`ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. `/mnt/data` ë˜ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ë°°ì¹˜í•´ì£¼ì„¸ìš”.")

    # ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš” (ì „ì²´ ìµœì‹ ì›” ê¸°ì¤€)
    if skills is not None and "skill" in skills.columns:
        kdf = skills.copy()
        if "month" in kdf.columns and kdf["month"].notna().any():
            top_month = kdf["month"].max()
            if (kdf["month"] == top_month).any():
                kdf = kdf[kdf["month"] == top_month]
        try:
            if "job_count" in kdf.columns:
                top_skills = (
                    kdf.groupby("skill")["job_count"]
                    .sum().sort_values(ascending=False).head(10).index.tolist()
                )
            else:
                top_skills = kdf["skill"].value_counts().head(10).index.tolist()
        except Exception:
            top_skills = []
        if top_skills:
            lines.append(f"- ìµœê·¼ ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”: {', '.join(top_skills)}")
    else:
        lines.append("- `skills_analysis.csv`ë¥¼ ì°¾ì§€ ëª»í•´ ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    return "\n".join(lines) + "\n\n> *ì°¸ê³ : ë°ì´í„°ëŠ” ë¡œì»¬ CSV ê¸°ì¤€ ìš”ì•½ì´ë©°, ë” ìì„¸í•œ ì›¹ ë¦¬ì„œì¹˜ëŠ” ì„ íƒì ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.*"

# ================= ì›¹ ë™í–¥/ê¸°ì—… ì¸ì¬ìƒ ìˆ˜ì§‘(ì„ íƒ) =================

def search_web(query: str, topk: int = 5) -> List[Dict[str, str]]:
    """ê°„ë‹¨í•œ ì›¹ ê²€ìƒ‰: SERPAPI ë˜ëŠ” Bing APIê°€ ìˆìœ¼ë©´ ì‚¬ìš©. ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸.
    ë°˜í™˜: [{title, url, snippet}]"""
    res: List[Dict[str, str]] = []
    serp_key = os.getenv("SERPAPI_API_KEY")
    bing_key = os.getenv("BING_API_KEY")
    try:
        if serp_key:
            params = {
                "engine": "google",
                "q": query,
                "api_key": serp_key,
                "num": topk,
                "hl": "ko"
            }
            r = requests.get("https://serpapi.com/search.json", params=params, timeout=15)
            j = r.json()
            for it in j.get("organic_results", [])[:topk]:
                res.append({
                    "title": it.get("title", ""),
                    "url": it.get("link", ""),
                    "snippet": it.get("snippet", "")
                })
        elif bing_key:
            headers = {"Ocp-Apim-Subscription-Key": bing_key}
            r = requests.get(
                "https://api.bing.microsoft.com/v7.0/search",
                params={"q": query, "count": topk, "mkt": "ko-KR"},
                headers=headers,
                timeout=15,
            )
            j = r.json()
            for it in j.get("webPages", {}).get("value", [])[:topk]:
                res.append({
                    "title": it.get("name", ""),
                    "url": it.get("url", ""),
                    "snippet": it.get("snippet", "")
                })
        return res
    except Exception:
        return res

def fetch_and_summarize(urls: List[str]) -> str:
    """ê°„ë‹¨ í¬ë¡¤ë§ í›„ ìš”ì•½ (LLM ì‚¬ìš© ê°€ëŠ¥ ì‹œ)."""
    texts = []
    for u in urls[:5]:
        try:
            html = requests.get(u, timeout=15).text
            soup = BeautifulSoup(html, "html.parser")
            t = " ".join([p.get_text(" ", strip=True) for p in soup.find_all(["p", "li"])])
            texts.append(textwrap.shorten(t, 3000))
        except Exception:
            pass
    joined = "\n\n".join(texts) if texts else ""
    if not joined:
        return "(ì›¹ í˜ì´ì§€ì—ì„œ ìš”ì•½í•  í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.)"
    if LLM_OK and os.getenv('OPENAI_API_KEY'):
        sys = "ë„ˆëŠ” ë¦¬ì„œì¹˜ ìš”ì•½ê°€ë‹¤. í•œêµ­ì–´ë¡œ 5ê°œ ë¶ˆë¦¿, 5ì¤„ ì´í•˜ ìš”ì•½ìœ¼ë¡œ ì •ë¦¬í•˜ë¼."
        tmpl = ChatPromptTemplate.from_messages([
            ("system", sys), ("human", "ë‹¤ìŒ ìë£Œë¥¼ ìš”ì•½:\n{t}")
        ])
        out = LLMChain(llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.2), prompt=tmpl).invoke({"t": joined})
        return out.get("text", str(out))
    return joined[:1500]

def company_persona_and_requirements(company: str, role: str) -> Dict[str, str]:
    """íšŒì‚¬ ì¸ì¬ìƒ/ìš”êµ¬ì—­ëŸ‰ ìš”ì•½ (ì›¹ ê²€ìƒ‰ í‚¤ ì„¤ì • ì‹œ)."""
    result = {"ì¸ì¬ìƒ": "", "ìš”êµ¬ì—­ëŸ‰": "", "ì¶œì²˜": []}
    if not HTTP_OK:
        return result
    queries = [
        f"{company} ì¸ì¬ìƒ site:co.kr OR site:com OR site:kr",
        f"{company} ì±„ìš© {role} ìê¸°ì†Œê°œì„œ",
        f"{company} core values culture"
    ]
    urls = []
    for q in queries:
        hits = search_web(q, topk=5)
        urls.extend([h["url"] for h in hits])
        result["ì¶œì²˜"].extend(hits)
    urls = list(dict.fromkeys([u for u in urls if u]))  # unique
    summary = fetch_and_summarize(urls)
    result["ì¸ì¬ìƒ"] = summary
    # ì¶”ê°€ì ìœ¼ë¡œ skills_dfê°€ ìˆë‹¤ë©´ role ê´€ë ¨ ìƒìœ„ ê¸°ìˆ  í‚¤ì›Œë“œë¥¼ ì¶”ë ¤ ì œì•ˆ
    if skills is not None and 'skill' in skills.columns:
        top_month = skills['month'].max() if 'month' in skills.columns else None
        kdf = skills.copy()
        if top_month:
            kdf = kdf[kdf['month'] == top_month]
        top_skills = (
            kdf.groupby('skill')['job_count'].sum().sort_values(ascending=False).head(10).index.tolist()
            if 'job_count' in kdf.columns else kdf['skill'].value_counts().head(10).index.tolist()
        )
        result["ìš”êµ¬ì—­ëŸ‰"] = "ìµœê·¼ ìˆ˜ìš” ìƒìœ„ ê¸°ìˆ  ì˜ˆì‹œ: " + ", ".join(top_skills)
    return result

# ================= ê¸°ë³¸ ê°€ì´ë“œ (ë§¨ ì•„ë˜ ìƒˆ ê¸°ëŠ¥ 2ì¤„ ì¶”ê°€) =================
GUIDE = """ğŸ“ **AI ìê¸°ì†Œê°œì„œ ì½”ì¹˜ ì‚¬ìš© ê°€ì´ë“œ**
1) **ìì†Œì„œ í‰ê°€ íƒ­**ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë¶™ì—¬ë„£ê³ , íšŒì‚¬/ì§ë¬´ë¥¼ ì…ë ¥ í›„ **í‰ê°€ ì‹¤í–‰**
   - ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ + LLM ê°œì„ ì•ˆ + ìŠ¤í‚¬ ë§¤ì¹­ í‘œ ì œê³µ
2) **íŠ¸ë Œë“œ/ê¸°ì—… íƒ­**ì—ì„œ íšŒì‚¬/ì§ë¬´ ì…ë ¥ â†’ ìµœì‹  ê³µê³ /ê¸°ìˆ  ì¶”ì´ì™€ íšŒì‚¬ ì¸ì¬ìƒ ìš”ì•½
3) ì¢Œì¸¡ **ì„¤ì •**ì—ì„œ OpenAI ë° (ì„ íƒ) SERP/Bing í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ì›¹ ìš”ì•½ ê¸°ëŠ¥ í™œì„±í™”
4) (ì„ íƒ) Tableau Public ë§í¬ê°€ ìˆë‹¤ë©´ íƒ­ í•˜ë‹¨ì— ì„ë² ë“œí•˜ì—¬ íŒ€ê³¼ ê³µìœ  ê°€ëŠ¥

ğŸ“¡ **ìƒˆ ê¸°ëŠ¥ ì•ˆë‚´**
ì±„íŒ…ì°½ì— **"ë‚´ê°€ (íšŒì‚¬ëª…)ì˜ ìì†Œì„œì— ëŒ€í•œ ë°ì´í„°ë¥¼ ì–»ê³  ì‹¶ì–´"** ë¼ê³  ì…ë ¥í•˜ë©´,
ë¡œì»¬ CSVë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ë‹¹ ê¸°ì—…ì˜ **ì±„ìš©/ê¸°ìˆ  ìˆ˜ìš” ìš”ì•½**ì„ ë°”ë¡œ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤!
"""

# ================= ì‚¬ì´ë“œë°” (ì„¤ì •) =================
with st.sidebar:
    st.header("ì„¤ì •")
    api_key = st.text_input("OpenAI API Key", value=os.getenv("OPENAI_API_KEY", ""), type="password")
    if api_key:
        os.environ["OPENAI_API_KEY"] = api_key
    serp_key = st.text_input("SERPAPI_API_KEY (ì„ íƒ)", value=os.getenv("SERPAPI_API_KEY", ""), type="password")
    if serp_key:
        os.environ["SERPAPI_API_KEY"] = serp_key
    bing_key = st.text_input("BING_API_KEY (ì„ íƒ)", value=os.getenv("BING_API_KEY", ""), type="password")
    if bing_key:
        os.environ["BING_API_KEY"] = bing_key

    st.markdown("---")
    st.caption(f"ë°ì´í„° ê²½ë¡œ: **{DATA_DIR}** (ìë™ ê°ì§€)")
    st.caption("CSV: job_market.csv, macro_indicators.csv, skills_analysis.csv, tech_trends.csv")

st.markdown(f"<div class='header'><b>AI ìê¸°ì†Œê°œì„œ ì½”ì¹­ +</b></div>", unsafe_allow_html=True)

st.markdown(GUIDE)

# ================= íƒ­ =================
tab_chat, tab_eval, tab_trend = st.tabs(["ğŸ’¬ ëŒ€í™”", "ğŸ§­ ìì†Œì„œ í‰ê°€", "ğŸ“ˆ íŠ¸ë Œë“œ/ê¸°ì—…"])

# --------- ğŸ’¬ ëŒ€í™” ---------
with tab_chat:
    st.subheader("ì¼ë°˜ ì½”ì¹­ ëŒ€í™”")
    user_q = st.text_area("ë©”ì‹œì§€ ì…ë ¥", placeholder="ì˜ˆ: ì‹ ì… í”„ë¡ íŠ¸ì—”ë“œ ì§€ì›, ì„±ì¥ê²½í—˜ ë¬¸ë‹¨ í”¼ë“œë°±")
    if st.button("ë‹µë³€ ìƒì„±", type="primary"):
        # (NEW) ê¸°ì—… ë°ì´í„° ì§ˆì˜ íŒ¨í„´ ìš°ì„  ì‘ë‹µ â€” UI ì˜í–¥ ì—†ìŒ, í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥
        company_name = try_parse_company_query(user_q)
        if company_name:
            st.markdown(summarize_company_from_csvs(company_name))
        else:
            if not LLM_OK or not os.getenv("OPENAI_API_KEY"):
                st.info("OpenAI í‚¤ê°€ ì—†ê±°ë‚˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ ê¸°ë³¸ ê°€ì´ë“œë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
                st.write(GUIDE)
            else:
                sys = "ì „ë¬¸ ìê¸°ì†Œê°œì„œ ì½”ì¹˜. ê°„ê²°í•˜ê³  ì‹¤ìš©ì ì¸ ì˜ˆì‹œì™€ êµ¬ì¡°ë¥¼ ì œì‹œ."
                tmpl = ChatPromptTemplate.from_messages([
                    ("system", sys), ("human", "{q}")
                ])
                out = LLMChain(llm=ChatOpenAI(model="gpt-4o-mini", temperature=0.5), prompt=tmpl).invoke({"q": user_q})
                st.markdown(out.get("text", str(out)))

# --------- ğŸ§­ ìì†Œì„œ í‰ê°€ ---------
with tab_eval:
    st.subheader("ìì†Œì„œ í‰ê°€ & ê°œì„ ")
    colL, colR = st.columns([1.2, 1])
    with colL:
        uploaded = st.file_uploader("ìì†Œì„œ íŒŒì¼(txt/docx)", type=["txt", "docx"]) 
        text = st.text_area("ë˜ëŠ” ì—¬ê¸° ë¶™ì—¬ë„£ê¸°", height=260)
        if uploaded and not text:
            text = read_text_from_upload(uploaded)
        role = st.text_input("ì§€ì› ì§ë¬´ (ì˜ˆ: í”„ë¡ íŠ¸ì—”ë“œ)"
                             )
        company = st.text_input("ì§€ì› íšŒì‚¬ (ì˜ˆ: ë„¤ì´ë²„)")
        tone = st.selectbox("í†¤", ["ì „ë¬¸ì ", "ì¹œê·¼í•œ", "ê²©ì‹ ìˆëŠ”", "ë‹´ë°±í•œ"], index=0)
        length = st.slider("ì¶œë ¥ ê¸¸ì´(ì)", 400, 1500, 900)
        run = st.button("í‰ê°€ ì‹¤í–‰", type="primary")

    with colR:
        st.markdown("**í‰ê°€ ì§€í‘œ**")
        st.caption("ì„±ê³¼Â·í–‰ë™Â·STARÂ·ê¸¸ì´Â·ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€Â·êµ°ë”ë”ê¸°")
        placeholder_metrics = st.empty()
        improved_box = st.empty()

    if run and text.strip():
        with st.spinner("í‰ê°€ ì¤‘â€¦"):
            scores = compute_resume_scores(text, role, company, skills)
        with placeholder_metrics.container():
            if VIZ_OK and PANDAS_OK:
                df_score = pd.DataFrame({"í•­ëª©":["ì´ì ","ì„±ê³¼","í–‰ë™","STAR","ê¸¸ì´","ìŠ¤í‚¬","ê°ì "],
                                         "ì ìˆ˜":[scores['ì´ì (0-100)'],scores['ì„±ê³¼(ìˆ«ì)ë°€ë„']*100,scores['í–‰ë™ì„±']*100,
                                                 scores['STARêµ¬ì¡°']*100,scores['ê¸¸ì´ì ì •']*100,scores['ìŠ¤í‚¬ì»¤ë²„ë¦¬ì§€']*100,
                                                 scores['êµ°ë”ë”ê¸°ê°ì ']*100]})
                fig = px.bar(df_score, x="í•­ëª©", y="ì ìˆ˜", title="í‰ê°€ ê²°ê³¼(%)", range_y=[0,100])
                st.plotly_chart(fig, use_container_width=True)
            st.json(scores)

        with st.spinner("ê°œì„ ì•ˆ ì‘ì„±â€¦"):
            improved = llm_improve(text, role, company, tone, length)
        with improved_box.container():
            st.markdown("### âœï¸ ê°œì„ ì•ˆ")
            st.markdown(improved)

        # ìŠ¤í‚¬ ë§¤ì¹­ í‘œ
        if PANDAS_OK and skills is not None:
            cov, matched = skill_coverage(text, skills)
            st.markdown("---")
            st.markdown("**ìŠ¤í‚¬ ë§¤ì¹­(ìµœê·¼ ìˆ˜ìš” ê¸°ì¤€)**")
            st.write(f"ì»¤ë²„ë¦¬ì§€: {cov*100:.1f}% / ë§¤ì¹­: {', '.join(matched) if matched else '(ì—†ìŒ)'}")
            if VIZ_OK:
                top_month = skills['month'].max() if 'month' in skills.columns else None
                kdf = skills.copy()
                if top_month:
                    kdf = kdf[kdf['month']==top_month]
                if 'job_count' in kdf.columns:
                    kdf = kdf.groupby('skill')['job_count'].sum().sort_values(ascending=False).head(15).reset_index()
                    st.altair_chart(
                        alt.Chart(kdf).mark_bar().encode(x='job_count', y=alt.Y('skill', sort='-x'))
                        .properties(height=380, title=f"{top_month or ''} ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”"), use_container_width=True
                    )

# --------- ğŸ“ˆ íŠ¸ë Œë“œ/ê¸°ì—… ---------
with tab_trend:
    st.subheader("ìµœì‹  ìì†Œì„œ ë™í–¥ + ê¸°ì—… ì¸ì¬ìƒ/ìš”êµ¬ì—­ëŸ‰")
    c1, c2 = st.columns(2)
    with c1:
        t_company = st.text_input("íšŒì‚¬ëª…", key="trend_company")
        t_role = st.text_input("ì§ë¬´", key="trend_role")
        do_crawl = st.button("ì›¹ ë¦¬ì„œì¹˜ ì‹¤í–‰")
    with c2:
        tableau_link = st.text_input("(ì„ íƒ) Tableau Public ë§í¬ ì„ë² ë“œ")

    if tableau_link:
        st.markdown("---")
        st.markdown("**Tableau Public**")
        st.components.v1.iframe(tableau_link, height=520)

    # ë¡œì»¬ ë°ì´í„° ì¸ì‚¬ì´íŠ¸
    st.markdown("---")
    st.markdown("### ğŸ“Š ë¡œì»¬ ë°ì´í„° ì¸ì‚¬ì´íŠ¸")
    if PANDAS_OK and job_market is not None:
        if VIZ_OK and 'posted_date' in job_market.columns:
            try:
                jdf = job_market.copy()
                jdf['posted_date'] = pd.to_datetime(jdf['posted_date'], errors='coerce')
                ts = jdf.groupby(pd.Grouper(key='posted_date', freq='M'))['job_code'].nunique().reset_index()
                ts.columns = ['ì›”', 'ê³µê³ ìˆ˜']
                st.altair_chart(
                    alt.Chart(ts).mark_line(point=True).encode(x='ì›”:T', y='ê³µê³ ìˆ˜:Q').properties(height=280, title='ì›”ë³„ ì±„ìš©ê³µê³  ì¶”ì´'),
                    use_container_width=True
                )
            except Exception:
                pass
    if PANDAS_OK and skills is not None and VIZ_OK:
        top_month = skills['month'].max() if 'month' in skills.columns else None
        kdf = skills.copy()
        if top_month:
            kdf = kdf[kdf['month']==top_month]
        if 'job_count' in kdf.columns:
            kdf = kdf.groupby('skill')['job_count'].sum().sort_values(ascending=False).head(15).reset_index()
            st.altair_chart(
                alt.Chart(kdf).mark_bar().encode(x='job_count', y=alt.Y('skill', sort='-x'))
                .properties(height=360, title=f"{top_month or ''} ìƒìœ„ ê¸°ìˆ  ìˆ˜ìš”"), use_container_width=True
            )

    # ì›¹ ë¦¬ì„œì¹˜(ì„ íƒ)
    if do_crawl and t_company:
        if not HTTP_OK:
            st.warning("requests/bs4 ë¯¸ì„¤ì¹˜ë¡œ ì›¹ ë¦¬ì„œì¹˜ë¥¼ ìƒëµí•©ë‹ˆë‹¤. 'pip install requests beautifulsoup4' ì„¤ì¹˜ í›„ ì¬ì‹œë„")
        else:
            with st.spinner("íšŒì‚¬ ì¸ì¬ìƒ/ìš”êµ¬ì—­ëŸ‰ ìˆ˜ì§‘ ì¤‘â€¦"):
                info = company_persona_and_requirements(t_company, t_role)
            if info.get("ì¸ì¬ìƒ"):
                st.markdown("### ğŸ¢ ì¸ì¬ìƒ ìš”ì•½")
                st.write(info["ì¸ì¬ìƒ"]) 
            if info.get("ìš”êµ¬ì—­ëŸ‰"):
                st.markdown("### âœ… ìš”êµ¬ì—­ëŸ‰(íŠ¸ë Œë“œ ê¸°ë°˜ ì œì•ˆ)")
                st.write(info["ìš”êµ¬ì—­ëŸ‰"]) 
            if info.get("ì¶œì²˜"):
                st.markdown("#### ğŸ”— ì°¸ê³  ë§í¬")
                for s in info["ì¶œì²˜"][:8]:
                    st.markdown(f"- [{s.get('title','(ì œëª©ì—†ìŒ)')}]({s.get('url','')}) â€” {s.get('snippet','')}")

# ================= ë‚´ë³´ë‚´ê¸°(ëŒ€í™” ì €ì¥) ì˜ˆì‹œ =================
def export_text(name: str, content: str) -> Tuple[str, bytes, str]:
    now = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    fname = f"{name}_{now}.txt"
    return fname, content.encode('utf-8'), 'text/plain'

st.markdown("---")
with st.expander("ğŸ“¥ ëŒ€í™”/ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"):
    export_name = st.text_input("íŒŒì¼ ì´ë¦„", value="resume_coach_result")
    export_text_content = st.text_area("ë‚´ë³´ë‚¼ í…ìŠ¤íŠ¸", value="ìš”ì•½/ê°œì„ ì•ˆ/ìŠ¤ì½”ì–´ ë“±ì„ ë³µì‚¬í•´ ë‘ì„¸ìš”.")
    if st.button("TXTë¡œ ì €ì¥"):
        fname, data, mime = export_text(export_name, export_text_content)
        st.download_button("ë‹¤ìš´ë¡œë“œ", data=data, file_name=fname, mime=mime)

# ================= ë =================
